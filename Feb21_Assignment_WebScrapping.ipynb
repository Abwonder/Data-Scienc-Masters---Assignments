{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8101039",
   "metadata": {},
   "source": [
    "### Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50967d",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data\n",
    "\n",
    "__Weg scraping__\n",
    "\n",
    "Web scraping is the automated process of extracting data from websites. It involves using a program or software to collect and analyze data from various web pages, and then organize it in a structured format such as a spreadsheet or database.\n",
    "\n",
    "Reasons for using Web scrapping:\n",
    "1. To Automate the process of data collect that are tidious of error prone when done manually.\n",
    "2. It is used when huge amount of data for either market research, analysis or price comparation.\n",
    "3. When data need span across multiple website and web pages.\n",
    "\n",
    "Give three areas where Web Scraping is used to get data:\n",
    "\n",
    "E-commerce: Web scraping is often used by businesses to gather pricing information from competitor websites. This data can then be used to make informed pricing decisions and stay competitive in the market.\n",
    "\n",
    "Research and analysis: Researchers and analysts can use web scraping to gather data for their studies, such as social media sentiment analysis, stock market trends, or news articles. This data can then be analyzed to identify patterns, trends, and insights.\n",
    "\n",
    "Marketing: Web scraping can be used to collect customer reviews, ratings, and feedback from various websites. This information can then be used to improve products or services and to create targeted marketing campaigns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36e46b",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "\n",
    "Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "\n",
    "Below are few of the different method for extracting data:\n",
    "\n",
    "Manual Scraping: This involves manually copying and pasting data from web pages into a spreadsheet or database. This method is only suitable for small amounts of data and is time-consuming.\n",
    "\n",
    "XPath and CSS Selectors: XPath and CSS selectors are used to locate specific elements within a web page's HTML code. This method is useful for extracting data from specific parts of a page.\n",
    "\n",
    "Regular Expressions: Regular expressions are used to extract specific patterns of text from web pages. This method is useful for extracting data that follows a specific format, such as phone numbers or email addresses.\n",
    "\n",
    "Web Scraping Tools and Libraries: There are various web scraping tools and libraries available, such as BeautifulSoup, Scrapy, and Selenium. These tools automate the scraping process and make it easier to extract data from multiple web pages.\n",
    "\n",
    "APIs: Some websites provide an API (Application Programming Interface) that allows developers to access and extract data in a structured format. This method is useful for extracting large amounts of data and is often faster than other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b834b34",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "Beautiful Soup is a Python library used for web scraping purposes. It allows developers to parse HTML and XML documents and extract the relevant data from them.\n",
    "\n",
    "Beautiful Soup is used for web scraping because it makes it easy to navigate and search HTML and XML documents. It can handle poorly formatted documents and adjust the parsing rules to extract data from them. Beautiful Soup also provides a variety of parsing methods and filters to extract specific data elements based on their attributes or tags.\n",
    "\n",
    "Below are the different features of Beautiful Soup:\n",
    "\n",
    "* Easy navigation of HTML and XML documents\n",
    "* Support for different parsing methods\n",
    "* Robust error handling and adjustment to poorly formatted documents\n",
    "* Ability to extract specific data elements based on tags and attributes\n",
    "* Integration with other Python libraries and tools for data analysis and manipulation.\n",
    "\n",
    "Overall, Beautiful Soup is a powerful tool for web scraping and data extraction, and it is widely used in the Python development community for a variety of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3f727",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Q4. Why is flask used in this Web Scraping project?\n",
    "\n",
    "Flask was used to route the path of the python code to an API so that it can be render on URL and accessed either using POST or GET. It was used to render html page that enable client to enter the input over web application and also render the output on html. It was used to aid storing data over the url to the database and also helped to work with deployment on AWS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c136a",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "1. codepipeline\n",
    "2. Beanstalk\n",
    "\n",
    "AWS Elastic Beanstalk is a platform-as-a-service (PaaS) offered by Amazon Web Services (AWS) that makes it easy to deploy, manage, and scale web applications and services. It abstracts away the underlying infrastructure and provides a simple and streamlined experience for developers to focus on writing code and not worry about the infrastructure\n",
    "BENEFITS:\n",
    "A. Easy deployment: Elastic Beanstalk simplifies the process of deploying web applications and services by providing pre-configured platforms and environments for various programming languages and frameworks.\n",
    "B. Scalability: Elastic Beanstalk automatically scales the infrastructure based on the demand of the application. It can automatically provision additional resources and adjust capacity to handle spikes in traffic or demand.\n",
    "\n",
    "\n",
    "AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service provided by Amazon Web Services (AWS) that helps developers automate the release process of their applications. It provides a fully managed workflow that automates the building, testing, and deployment of code changes.\n",
    "BENEFITS:\n",
    "A. Automated workflow: AWS CodePipeline allows developers to define a fully automated workflow that includes building, testing, and deploying code changes. This reduces manual errors and speeds up the release process.\n",
    "\n",
    "B. Integration with AWS services: AWS CodePipeline integrates with other AWS services such as AWS CodeBuild, AWS CodeDeploy, AWS Lambda, and Amazon S3, to create a complete CI/CD pipeline.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
